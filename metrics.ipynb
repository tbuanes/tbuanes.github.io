{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2b7edd59",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Evaluation metrics\"\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf21ed5a",
      "metadata": {},
      "source": [
        "Before discussing the how to chose an appropriate metric for the problem at hand, let's start by defining the most commonly used metrics for classification and regression problems, respectively. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d312f163",
      "metadata": {},
      "source": [
        "## Metrics for binary classification\n",
        "\n",
        "An ML model typically give out a _score_ for each instance which is the basis for the classification. Often this score is normalised to be in the range [0,1] where we take numbers closer to 0 to mean classified as belonging to the negative class, and numbers closer to 1 mean classified as belonging to the positive class. Often, but not necessarily, the threshold is placed at 0.5. \n",
        "\n",
        "The most commonly used metrics for classification are based on the _confusion matrix_. The confusion matrix for binary classification is a $2\\times 2$ matrix categorising the classified events as\n",
        "\n",
        "* True positive (TP) if the instance was classified as belonging to the _positive_ class and this was _correct_.\n",
        "* True negative (TN) if the instance was classified as belonging to the _negative_ class and this was _correct_.\n",
        "* False positive (FP) if the instance was classified as belonging to the _positive_ class and this was _incorrect_.\n",
        "* False negative (FN) if the instance was classified as belonging to the _negative_ class and this was _incorrect_.\n",
        "\n",
        "There is no universally adopted convention for whether rows or columns should present the actual value, so it is important that the axes on the confusion matrix are labelled.\n",
        "\n",
        "![](./images/confusionmatrix.png)\n",
        "\n",
        "The numbers in the confusion matrix may be presented in different ways: Commonly the absolute numbers for each category is presented, but it is often beneficial to also include relative numbers. The normalization of the relative numbers may differ, so pay attention! The numbers for all of the matrix may sum to 1, or each row or column may sum to 1.\n",
        "\n",
        "![](./images/confusionmatrixNum.png)\n",
        "\n",
        "Based on the numbers from the confusion matrix, we can define several scalars which are measures of performance. Since we are reducing the $2\\times2$ matrix into a scalar, some information is necessarily lost. With an appropriate choice of metric, this loss of information isn't a problem, but choosing a metric which doesn't fit the problem at hand will yield a misleading measure of performance.\n",
        "\n",
        "- **Accuracy** is a measure of the fraction of correctly units\n",
        "$$\\text{Accuracy} = \\frac{TP + TN}{P + N} = \\frac{TP + TN}{TP + FP  + TN + FN}$$\n",
        "- **Precision** is a measure of how large a fraction classified as positive was correctly classified\n",
        "$$\\text{Precision} = \\frac{TP}{TP+FP}$$\n",
        "- **True positive rate (TPR)**, **recall** or **sensitivity** is a measure of how large a fraction of the positive class was correctly classified\n",
        "$$\\text{TPR} = \\frac{TP}{P} = \\frac{TP}{TP + FN}$$\n",
        "- **True negative rate (TNR)**, **Specificity** is a measure of how large a fraction of the negative class was correctly classified\n",
        "$$\\text{TNR} = \\frac{TN}{N} = \\frac{TN}{TN + FP}$$\n",
        "- **F~1~ score** is the most commonly used member of a more general class of measures (F~$\\beta$~ score) and is defined as a mean of precision and recall\n",
        "$$\\text{F}_1 = 2\\frac{\\text{precision}\\cdot\\text{recall}}{\\text{precision+recall}} = \\frac{2TP}{2TP + FP + FN}$$\n",
        "By choosing $\\beta \\neq 1$ one can give more weigth to either precision or recall\n",
        "$$\\text{F}_\\beta = (1+\\beta^2)\\frac{\\text{precision}\\cdot\\text{recall}}{(\\beta^2\\cdot\\text{precision)+recall}} = \\frac{(1+\\beta^2)TP}{(1+\\beta^2)TP + FP + \\beta^2FN}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "212fb49b",
      "metadata": {},
      "source": [
        "### Choosing a metric\n",
        "\n",
        "All the above metrics are valid measures-of-goodness, but not all make sense in all cases. For a trivial example, consider *precision*. If this metric is used in the maintenance example from the [utility section](lossMetricUtility.ipynb#sec-utility), the result would be very bad indeed. Since the metric prioritises a high purity among the instances classified as positive, but does not pay any regard to the instances classified as negative, a large false negative rate is likely. But this was exactly what needed to be avoided in this case, since a false negative would lead to a breakdown of the machinery and thus incur a large cost.\n",
        "\n",
        "A slightly less obvious, but quite common and important, example of a bad choise of metric occurs when we have a large imbalance between the classes in the sample we want to classify. Say, positive class only constitute 1 permille of the total sample. If we use *accuracy* as our metric we would probably end up with a model that classifies all instances as negative, since this model would be 99.9% accurate. However, despite of the very high accuracy, the model is completely useless since none of the interesting instances are found."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "628ce0fb",
      "metadata": {},
      "source": [
        "::: {.callout-note title=\"Exercise\"}\n",
        "Your task is to make a machine learning algorithm that will classify a large set of chemical compounds as being either good candidates for becoming a new medicine (positive) or uninteresting for medical use (negative). The selected compounds will be taken into further tests to prove or disprove their applicability as a medicine. Find suitable metrics in the given scenarios:\n",
        "\n",
        "1. Further testing is cheap, and the potential revenue of the new medicine is large.\n",
        "2. Further testing is expensive, and the potiential revenue of the new medicine is modest.\n",
        "3. Further testing is expensive, and the potenial revenue of the new mecdicine is large. \n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d65aad97",
      "metadata": {},
      "source": [
        "::: {.callout-note title=\"Exercise\"}\n",
        "In a search for [dark matter](https://en.wikipedia.org/wiki/Dark_matter) a large dataset is collected. The vast majority (possibly all) of the instances in the dataset does not contain a the signature for dark matter. Imagine making a machinelearning algorithm tasked with finding instances which are candidates to being evidence of dark matter. Let an instance with a dark matter signature be the positive class. A successful algorithm should select a sample of instances with a large positive-to-negative ratio, but the sample should not be too small since this would mean that statistical fluctuations in the sampling degrades the result.\n",
        "\n",
        "1. Why is accuracy, precision and recall bad choises of metric in this scenario?\n",
        "2. Propose a good metric. Can one of the \"standard metrics\" be used, or would it be better to device a new one?\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77d1b295",
      "metadata": {},
      "source": [
        "## Metrics for regression\n",
        "\n",
        "Metrics for regression are typical an aggregate of the deviation between model prediction and correct value for all instances in the data set used for evaluation. Since both predicting a too high and a too low value is undesirable, one need to apply a function such that both cases contribute with a positive number in the aggregated value. In order to make the metric independent of number of instances used in the evaluation, one typically use the mean deviation.\n",
        "\n",
        "Let $\\hat{y}_i$ be the predicted value corresponding to correct value $y_i$. \n",
        "\n",
        "* **Mean squared error (MSE)** is a mean of the squared deviations from the correct value\n",
        "$$\\text{MSE} = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i-y_i)^2$$\n",
        "\n",
        "* **Root mean squared error (RMSE)** is the square root of MSE\n",
        "$$\\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i-y_i)^2}$$\n",
        "\n",
        "* **Mean absolute error (MAE)** is the mean of the absolute values of the deviations from the correct value\n",
        "$$\\text{MAE} = \\frac{1}{N}\\sum_{i=1}^N |\\hat{y}_i - y_i|$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f3e2597",
      "metadata": {},
      "source": [
        "### Choosing a metric\n",
        "\n",
        "MSE and RMSE measure essentially the same thing, but there are nontheless important differences. MSE has the benefit that it is more easily differentiable, and is therefore a very good candidate for having the double use of both loss function and metric. RMSE, on the other hand, has - due to the square root after summing the squares - the same dimension as the input data. It is thus more easy to interpret what a particular value of RMSE means.\n",
        "\n",
        "::: {.callout-tip title=\"Example\"}\n",
        "Assume that we have a regression model that predicts the height of a person based on age, weight, gender, place of birth etc. Evaluating the model on a small sample of persons yields the following results\n",
        "\n",
        "|  $i$ | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |\n",
        "|----|----|----|----|----|----|----|----|----|----|----|\n",
        "| $y_i$ (m) | 1.68 | 1.72 | 1.89 | 1.78 | 1.94 | 1.76 | 1.43 | 1.82 | 1.67 | 1.72 |\n",
        "| $\\hat{y}_i$ (m) | 1.65 | 1.73 | 1.94 | 1.81 | 1.90 | 1.76 | 1.55 | 1.81 | 1.63 | 1.70 | \n",
        "\n",
        "From this we can calculate\n",
        "$$ \\text{MSE} = 2.25\\cdot 10^{-3}~\\mathrm{m}^2 $$\n",
        "$$ \\text{RMSE} = 0.047~\\mathrm{m}$$\n",
        "Both metrics give a valide measure of how well the model fits the data, but only RMSE give us immediate information about how large a deviation we should expect from the model.\n",
        ":::\n",
        "\n",
        "MAE is an obvious metric to define since what we care about is the absolute value of the error, but since the absolute value function is not differentiable in $x=0$ there are some numerical challenges in using it - which is part of the explanation why MSE or RMSE is more commonly preferred. A fact which may be a benefit compared to (R)MSE is that MAE is less sensitive to outliers. MAE has, as RMSE, the same dimension as the input data and is thus equally easy to interpret.\n",
        "\n",
        "::: {.callout-tip title=\"Example\"}\n",
        "Using the same dataset as in the previous example, we calculate \n",
        "$$ \\text{MAE} = 0.035~\\mathrm{m}$$\n",
        "which, just as RMSE, can be interpreted as the \"typical magnitude\" of the deviations.\n",
        "\n",
        "If we choose to throw away the largest deviation ($i=7)$ as an outlier, we get the updated values\n",
        "$$\\text{RMSE}' = 0.030~\\mathrm{m}$$\n",
        "$$\\text{MAE}' = 0.026~\\mathrm{m}$$\n",
        "As expected, we se that RMSE change more than MAE when not including the outlier.\n",
        ":::\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
